1Ô∏è‚É£ MobileNetV3 CPU Requirements (Reality)
Model you‚Äôre using

MobileNetV3-Small

Input: 224 √ó 224

FP32 (default PyTorch)

No GPU

Single Inference Cost (CPU)

On a modern server CPU:

Metric	Value
Inference time	20‚Äì50 ms
CPU usage	~1 core burst
Memory	< 300 MB
Model size	~5‚Äì6 MB

‚û°Ô∏è Very lightweight.

2Ô∏è‚É£ Throughput Per CPU Core
Conservative numbers (production-safe)
CPU cores	Images/sec
1 vCPU	15‚Äì20
2 vCPU	30‚Äì40
4 vCPU	60‚Äì80
8 vCPU	120‚Äì160

This assumes:

Batch size = 1

FastAPI + PyTorch

No heavy preprocessing

3Ô∏è‚É£ Example: Flipkart-Scale Scenarios
üß™ Small / POC

Use case: testing, pilot

2 vCPU

4 GB RAM

~2,000 images/hour

‚úÖ Cheap
‚úÖ Easy to deploy

üöÄ Medium Scale

Use case: moderate listing volume

8 vCPU

16 GB RAM

~400K images/day

‚úÖ Good for category-wide scans

üè≠ Large Scale (Realistic Flipkart numbers)

Assume:

1 million listings/day

Average 2 images/listing

Total images/day = 2M

Required throughput:

2,000,000 images / 86,400 sec ‚âà 23 images/sec

üëâ Infra required:

2 √ó 4 vCPU pods (safe)

Or 1 √ó 8 vCPU pod

That‚Äôs it.
No GPU required.

4Ô∏è‚É£ With OCR Included (Important!)

OCR is more expensive than MobileNet.

Tesseract OCR cost:

~100‚Äì200 ms per image

CPU-heavy

Combined pipeline (Image + OCR):
Component	Time
MobileNet	30 ms
OCR	150 ms
Decision logic	~5 ms
Total	~185 ms

üëâ Effective throughput:

~5 images/sec per core

5Ô∏è‚É£ Recommended Production Setup
Minimum Safe Setup
Inference Service:
- 8 vCPU
- 16 GB RAM
- Autoscaling enabled


OCR Service:
- 8 vCPU
- 16 GB RAM

Split them ‚ùó
Never run OCR + CV in the same pod.

6Ô∏è‚É£ Cost Approximation (Cloud)
AWS / GCP equivalent
Instance	Monthly Cost (approx)
8 vCPU + 16 GB	‚Çπ18k‚Äì25k
16 vCPU	‚Çπ35k‚Äì45k

üëâ Much cheaper than GPU.

7Ô∏è‚É£ When CPU Is NOT Enough

You‚Äôll need GPU only if:

YOLO / DETR

Video streams

10M images/day

Latency < 10ms

Otherwise ‚Üí CPU is correct choice.
